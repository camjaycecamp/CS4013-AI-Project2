Zane Lesley
Cameron Campbell

QS1.1: Explain the implementation of ReflexAgent in multiAgents.py and how you improved it.
A1.1: On its own, ReflexAgent simply chose the legal move out of the ones available with the best score using the vanilla evaluationFunction â€” which is a glorified call to the successorGameState.getScore() function. If there were multiple moves with the same score, they were chosen from randomly. With my improvements, the evaluationFunction now finds the closest food pellet and ghost for each successor state, derives positive and negative scores with incentive weights from them, and uses the sum of those values as the score returned by the function. There are also additional incentives that influence the score, such as any state where Pacman collides with a ghost being given the worst possible score and any state where Pacman passes on a move being heftily penalized.

QS1.2: What is your value function and why do you think this estimation makes sense?
A1.2: My value function is V(state,action) = (successor state score) + (consumption incentive) + (ghost distance incentive) + (closest pellet incentive) + (passed move incentive). Successor state score is the base score taken from the successor state, a la the vanilla evaluationFunction. Consumption incentive adds 30 to the score on the condition that the move consumes a pellet. Ghost distance incentive adds the closest ghost's distance to the score or alternatively subtracts 9999999 from it if the state would result in Pacman colliding with a ghost. Closest pellet incentive subtracts the distance of the closest food pellet from the score. Finally, passed move incentive subtracts 750 from the score of states where no move is taken and adds 5 to states where a move is taken. This value function works well for Pacman, as it prioritizes stay away from ghosts, getting close to and consuming pellets, and staying on the move no matter what.

QS2.1: Explain your algorithm. Why do you think it is working?
A2.1: My algorithm allows Pacman to recursively search through several levels of depth to anticipate the actions of ghosts and act against them more efficiently for it. At each game state, getAction searches the state for all possible legal actions that can be taken by Pacman and derives possible successors of those actions up to a certain depth. Those successors are then assigned scores via the minimax function and the optimal score is updated as better scores are encountered before finally returning the optimal action associated with the optimal score when all legal actions are iterated. The minimax function evaluates the agent's type (Pacman or a ghost) and then calls the appropriate version of the minifyMaxify function, making sure to instead return the standard evaluationFunction if the state in question is a terminal state. The minifyMaxify function finds the best possible score out of the legal actions available to the agent, with Pacman seeking the highest score and the ghosts seeking the lowest score. The score finding for loop of the function recursively calls minimax and finds the agent's best respective score, iterating through all agents until circling back the first agent (Pacman). The best score out of all scores is then passed back to minify and by proxy getAction, which then compares it against the most optimal score so far and potentially updating it before choosing another Pacman action and repeating the process until all actions up to the depth are evaluated and an optimal action is chosen. I believe the algorithm is working because, not only does it seem to pass all Q2 tests and perform much better than the Q1 evaluationFunction algorithm, but watching the AI reveals that it seems unnaturally good at predicting the moves of the ghosts and acting accordingly. Pacman does eventually die to them before finishing, but he puts up a hell of a good fight for how aggressive the ghosts seem to be in their pathfinding.

QS3.1: The AlphaBetaAgent minimax values should be identical to the MinimaxAgent minimax values.  Explain why.
A3.1: 

QS3.2: explain your strategy for breaking a tie.
A3.2: 

QS4.1: Explain your Expectimax algorithm.
A4.1: My expectimax algorithm is a fork of the Q2 minimax code that overhauls the logic and adjusts it to work for expected values over minimum values. The getAction method utilizes nearly identical logic other than a few changes for type and scope consistency, calling the expectimax function to help find an optimal action. The expectimax function uses an improved ternary operator that only returns a terminal state or the expectifyMaxify function with no required input for isGhost. The expectifyMaxify function is heavily altered, ditching isGhost logic in its value assignment and ternary operators in favor of directly checking the index of the agent. The for loop of expectifyMaxify was altered from minifyMaxify to use an externally declared score list to store scores from a homogenized expectimax call, utilizing index and depth variables defined with modulo and ternary operators respectively. This logic entirely replaces the nested ternary operator from the minifyMaxify for loop, and the final calculation of the best score is calculated once the score list is fully populated and the for loop ends, using a ternary operator to either max the score for Pacman or find an average for the ghosts.

QS5.1: explain your new evaluation function, why do you think this new evaluation function is better compared to the previous one?
A5.1: 

